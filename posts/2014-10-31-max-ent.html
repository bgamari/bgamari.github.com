<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>bgamari.github.com - Maximum entropy optimization: A tutorial</title>
    <link rel="stylesheet" type="text/css" href="../css/default.css" />
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  </head>
  <body>
    <div id="container">
      <div id="content">
        <div id="sidebar">
          <nav>
            <h1>Navigation</h1>
            <ul>
              <li><a href="../index.html">Home</a></li>
              <li><a href="../posts.html">Blog-like thing</a></li>
              <li><a href="../research-agenda.html">Research agenda</a></li>
              <li><a href="../publications.html">Publications</a></li>
              <li><a href="../past-research.html">Past research</a></li>
              <li><a href="../media/cv.pdf">Curriculum Vitæ</a></li>
            </ul>
            <h1>Related Links</h1>
            <ul>
              <li><a href="http://goldnerlab.physics.umass.edu/wiki/">Research group</a></li>
              <li><a href="http://www.github.com/bgamari">Github</a></li>
            </ul>
          </nav>
        </div>
  
        <section id="main">
        <h1 id="maximum-entropy-optimization-a-tutorial">Maximum entropy optimization: A tutorial</h1>
<p>When faced with data produced by a unknown mixture of processes it is often useful to model observations with a quasi-continuous mixture of models.</p>
<p>When trying to model such observations the first tool that pops into many heads is maximum likelihood inference. Unfortunately, maximum likelihood inference does quite poorly in learning models with an unknown component count. Maximum entropy inference is a class of alternative inference methods which demand not only that the inferred model fit the observations well, but also that it is parsimonious in its structure. This brings with it the advantage of being better behaved in the face of heterogeneous mixtures.</p>
<p>In this series we will look at the problem of modelling mixtures. We will consider a range of inference methods, eventually turning to the seminal optimization method of Skilling and Bryan.</p>
<h2 id="a-concrete-problem">A concrete problem</h2>
<p>Let’s make this concrete with an example: one cold day you are standing by the side of the road waiting for a friend to pick you up. Being the inquisitive type, you begin tabulating busses as they arrive to help pass the time. You note that there are three colors of busses and record a separate list of times for each: one set <span class="math">\(T_k = \{ t^{(k)}_0, ..., t^{(k)}_{N_k} \}\)</span> for each color <span class="math">\(k \in \{\mathrm{blue}, \mathrm{red}, \mathrm{green} \}\)</span>. After observing what seems like hundreds of bus arrivals, your now quite late friend arrives. The two of you return home to make sense of your newly accquired body of data.</p>
<p>Your first step is to plot the arrival times themselves,</p>
<div class="figure">
<img src="../media/max-ent/arrivals.svg" alt="The first few hundred observed arrival times" /><p class="caption">The first few hundred observed arrival times</p>
</div>
<p>Hmmm, not so interesting. Perhaps the time between arrivals will be more informative,</p>
<p><span class="math">\[ \tau^{(k)}_i = t^{(k)}_{i+1} - t^{(k)}_i \]</span></p>
<p>Histogramming these you find something vaguely familiar,</p>
<div class="figure">
<img src="../media/max-ent/interarrivals.svg" alt="Observed inter-arrival times" /><p class="caption">Observed inter-arrival times</p>
</div>
<p>Noting the apparently exponential nature of the histogram, you postulate you might be working with a <a href="http://en.wikipedia.org/wiki/Poisson_process">Poisson process</a>. A Poisson process with rate <span class="math">\(\lambda\)</span> will produce inter-arrival times distributed as,</p>
<p><span class="math">\[P(\tau \:\vert\: \lambda) = \lambda^{-1} e^{-\tau \lambda}\]</span></p>
<p>You assume that the observed bus colors <span class="math">\(k\)</span> correspond to different bus lines, uncorrelated from one another and each characterized by its own rate <span class="math">\(\lambda_k\)</span>.</p>
<div class="note">
I should note that some suspension of disbelief is needed here: real busses are not Poissonian in nature. Individual arrivals are correlated by virtue of their schedules but let’s ignore this for the sake of the example; for our purposes at any given moment there is a constant probability <span class="math">\(p\)</span> that a bus will pass by.
</div>
<p>These two considerations allow us to construct a probability density over observations for a given line <span class="math">\(T_k\)</span> as a function of a rate parameter <span class="math">\(\lambda_k\)</span>,</p>
<p><span class="math">\[ P(T_k \:\vert\: \lambda_k) = \prod_{i=0}^{N_k - 1} P(\tau^{(k)}_i \:\vert\: \lambda)\]</span></p>
<p>By maximizing <span class="math">\(P(T_k \;\vert\; \lambda_k)\)</span> (or, more conveniently, <span class="math">\(\log P(T_k \;\vert\; \lambda_k)\)</span>) over <span class="math">\(\lambda_k\)</span> we arrive at the maximum likelihood estimate of the rate,</p>
<p><span class="math">\[\tilde\lambda_k = \frac{N_k}{\sum_{i=0}^{N_k - 1} \tau^{(k)}_i}\]</span></p>
<h2 id="the-model-selection-problem">The model selection problem</h2>
<p>Let’s now consider the same observations ignoring our class labels (the bus colors). Here we’ll denote the combined set of bus arrivals as <span class="math">\(T\)</span> (note the absence of the subscript). Is it possible to infer rate of the observed bus arrivals?</p>
<p>The answer to this question depends upon whether we have prior knowledge of the number of bus lines represented in our observations. If we know that we have observed three separate lines, we can carry out a likelihood maximization over</p>
<p><span class="math">\[P(T \;\vert\; \vec\lambda, \vec w) = \prod_{i=0}^{N - 1} \sum_{k=1}^3 P(\tau^{(k)}_i) \]</span></p>
<p>where <span class="math">\(\vec w = (w_0, ..., w_k)\)</span> is the vector of mixing weights (e.g. the relative number of busses on each line). For large sample sizes we would expect this optimization to converge to the true rates <span class="math">\(\tilde\lambda\)</span>.</p>
<p>If we do not know the underlying class count, however, we must learn it in our model. The most obvious way to accomplish this would be to introduce the number of classes <span class="math">\(K\)</span> as a latent model parameter.</p>
<p>This obvious path suffers from the issue of over-fitting: adding another component to the model will nearly always allow for an improved likelihood. This means that the inference will learn not only from the significant trends in the observations, it will also learn from statistical fluctuations and other sources of noise. Not only will the spurious clusters that result be uninformative but their presence may very well affect the parameter estimates of other more useful clusters..</p>
<p>There are a few ways to remedy this,</p>
<ul>
<li><p>Use our prior beliefs on the model parameters to further disambiguate from among the potential models. This is known as maximum a posteriori (MAP) inference.</p></li>
<li><p>Choose a better-behaved criterion for optimization and enforce goodness-of-fit as a constraint</p></li>
</ul>
<p>Here we will consider this second approach, choosing the entropy of the mixing distribution as our optimization objective.</p>
<h2 id="maximum-entropy-inference">Maximum entropy inference</h2>
<p>Another way to frame the above problem is to consider a discrete mixture over a pre-defined set of components,</p>
<p><span class="math">\[P(T \;\vert\; w_\lambda) = \sum_\lambda w_\lambda P(\tau^{(k)}_i \;\vert\; \lambda)\]</span></p>
<p>Here we consider the component parameters themselves (the values <span class="math">\(\lambda_k\)</span>) fixed; instead our inference must infer their mixing coefficients <span class="math">\(\{w_\lambda\}\)</span>. Note that if we allow a large enough number of components we can recover the generality of the previous model over any finite interval of <span class="math">\(\lambda\)</span>.</p>
<p>Maximum likelihood inference seeks to find the parameters <span class="math">\(\vec\theta\)</span> that maximize the likelihood of the observations under the model <span class="math">\(P(\mathrm{Obs} \vert \vec\theta)\)</span> (or, equivalently with some further assumptions, a goodness-of-fit measure such as <span class="math">\(\chi^2\)</span>). In contrast, maximum entropy inference looks for a mixing distribution <span class="math">\(p(\lambda)\)</span> over a given set of components <span class="math">\(\lambda\)</span> which encodes the least information. One common quantification of information content is the Shannon entropy,</p>
<p><span class="math">\[S(p) = \sum_\lambda p(\lambda) \log p(\lambda) \]</span></p>
<p>Merely looking for <span class="math">\(p(\lambda)\)</span> to maximize <span class="math">\(S(p)\)</span> is alone not useful as it will simply bring us to a uniform distribution, <span class="math">\(p(\lambda) = \mathrm{const}\)</span>. For this technique to be productive we also need to incorporate our observations into the inference. One typical way to do this is via a constraint on some goodness-of-fit measure <span class="math">\(G\)</span>. Namely, we are solving the optimization problem <span class="math">\(\arg \max_p S(p)\)</span> such that <span class="math">\(G(p) &lt; G^*\)</span> for some desired goodness-of-fit <span class="math">\(G^*\)</span>.</p>
<p>The mechanics of performing this optimization is a bit tricky for reasons that aren’t at first glance apparent: entropy is non-linear and the logarithmic scaling</p>
<p>In the early 1980s J. Skilling and R. Bryan were thinking about this problem in the context of astronomical image reconstruction.</p>
<p><a href="http://en.wikipedia.org/wiki/Principle_of_maximum_entropy">principle of maximum entropy</a>: given a set of prior beliefs, the model that best captures the state of knowledge is the one which,</p>
<ul>
<li>Has minimal information content</li>
<li>Agrees with observation to some tolerance</li>
</ul>
<p>The information content of a model is captured by the information entropy of its mixing distribution,</p>
<p><span class="math">\[H[P] = \]</span></p>
<h2 id="references">References</h2>
<ul>
<li>Skilling, J., &amp; Bryan, R. K. (1984). Maximum entropy image reconstruction: general algorithm. <em>Mon. Not. R. Astr. Soc.</em>, 211, 111—124.</li>
<li>Bryan, R. K., &amp; Skilling, J. (1980). Deconvolution by maximum entropy as illustrated by application to the jet of M87. <em>Mon. Not. R. Astr. Soc.</em>, 191.</li>
<li>Burch, S. F., Gull, S. F., &amp; Skilling, J. (1983). Image Restoration by a Powerful Maximum Entropy Method. <em>Computer Vision, Graphics and Image Processing</em>, 23, 113—128.</li>
<li>Gull, S. F., &amp; Skilling, J. (1984). Maximum entropy method in image processing. <em>IEE Proceedings, Part F</em>, 131(6), 646. doi:10.1049/ip-f-1.1984.0099</li>
<li>Livesey, A. K., Licinio, P., &amp; Delaye, M. (1986). Maximum entropy analysis of quasielastic light scattering from colloidal dispersions. <em>The Journal of Chemical Physics</em>, 84(9), 5102. doi:10.1063/1.450663</li>
<li>Langowski, J., &amp; Bryan, R. K. (1991). Maximum Entropy Analysis of Photon Correlation Spectroscopy Data Using a Bayesian Estimate for the Regulazation Parameter. <em>Macromolecules</em>, (Table I), 6346—6348.</li>
<li>Sibisi, S., Skilling, J., Brereton, R. G., Laue, E. D., &amp; Staunton, J. (1984). Maximum entropy signal processing in practical NMR spectroscopy. <em>Nature</em>, 311(4).</li>
<li>Sengupta, P., Garai, K., Balaji, J., Periasamy, N., &amp; Maiti, S. (2003). Measuring size distribution in highly heterogeneous systems with fluorescence correlation spectroscopy. <em>Biophysical Journal</em>, 84(3), 1977—84. doi:10.1016/S0006-3495(03)75006-1</li>
<li>MacKay, D. J. C. <em>Information theory, inference, and learning algorithms</em> Cambridge University Press (2003).</li>
</ul>
        </section>
      </div>

      <footer id="footer">
        <span>Generated by <a href="http://jaspervdj.be/hakyll/index.html">Hakyll</a>.</span>
      </footer>
    </div>
  </body>
</html>

